{"learning_rate": 5e-05, "batch_size": 2, "dropout": 0, "optimizer": "adam", "activation": "tanh", "regularization": 0, "preweight": 250, "layer_1": 100, "layer_2": 200, "train_frac": 0.75, "test_frac": 0.25, "runs_number": 10, "epochs": 200}