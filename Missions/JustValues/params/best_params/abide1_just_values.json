{"activation": "relu", "batch_size": 50.0, "dropout": 0.001879773866244, "epochs": 200.0, "layer_1": 10.0,
  "layer_2": 145.0, "learning_rate": 0.037334891450147, "optimizer": "SGD", "preweight": 6.0,
  "regularization": 0.017838862781314, "test_frac": 0.15, "train_frac": 0.7}