{"activation": "relu", "batch_size": 32.0, "dropout": 0.4, "layer_1": 128.0, "layer_2": 32.0, "learning_rate": 0.001, "numrec": 125.0, "preweight": 10.0, "regularization": 0.001}