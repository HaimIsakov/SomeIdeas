{
    "learning_rate":0.001,
    "batch_size":30,
    "dropout":0.1,
    "optimizer":"adam",
    "activation":"relu",
    "regularization": 0.0001,
    "layer_1":10,
    "layer_2":10
}
