{"test_frac": 0.15, "layer_1": 168, "train_frac": 0.7, "batch_size": 5, "preweight": 15, "activation": "relu", "learning_rate": 0.0140260329729161, "optimizer": "adam", "epochs": 200, "layer_2": 122, "regularization": 0.0017964503201694, "dropout": 0.3044709628918806}