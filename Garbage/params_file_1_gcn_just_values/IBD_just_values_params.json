{"batch_size": 30, "epochs": 200, "regularization": 0.0276155969654584, "layer_2": 91, "activation": "relu", "layer_1": 78, "test_frac": 0.15, "train_frac": 0.7, "dropout": 0.1252329687932416, "learning_rate": 0.0166265980208172, "optimizer": "adam", "preweight": 20}